{
  "temperature": 0,
  "prompt": "## Task\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\n\n### Events:\nUser edited \"untitled\":\n```diff\n@@ -249,3 +249,4 @@\n - Better performance for small batch sizes\n \n These developments will likely make running large models even more efficient in the future.\n+What do you think --precision=FP8_MM_V2\n\\ No newline at end of file\n\n```\n\nUser edited \"untitled\":\n```diff\n@@ -249,4 +249,5 @@\n - Better performance for small batch sizes\n \n These developments will likely make running large models even more efficient in the future.\n-What do you think --precision=FP8_MM_V2\n\\ No newline at end of file\n+What do you think --precision=FP8_MM_V2 means?\n+\n\n```\n\n\n\nUser edited \"untitled\":\n```diff\n@@ -272,3 +272,5 @@\n - Hardware-accelerated performance on newer NVIDIA GPUs (especially H100s which have specific FP8 support)\n \n It's a middle ground between full precision (FP32/FP16) and more aggressive quantization like 4-bit, offering a balance of performance and accuracy.\n+This is VLLM isn't it\n+\n\n```\n\n\n\nUser edited \"crates/collab/src/llm.rs\":\n```diff\n@@ -492,8 +492,6 @@\n             prompt: prompt.clone(),\n             max_tokens: 512,\n             temperature: 0.,\n-            prediction: None,\n-            rewrite_speculation: None,\n             // prediction: Some(fireworks::Prediction::Content {\n             //     content: params.input_focus_excerpt.unwrap(),\n             // }),\n\n```\n\nUser edited \"crates/collab/src/llm.rs\":\n```diff\n@@ -492,10 +492,10 @@\n             prompt: prompt.clone(),\n             max_tokens: 512,\n             temperature: 0.,\n-            // prediction: Some(fireworks::Prediction::Content {\n-            //     content: params.input_focus_excerpt.unwrap(),\n-            // }),\n-            // rewrite_speculation: Some(true),\n+            prediction: Some(fireworks::Prediction::Content {\n+                content: params.input_focus_excerpt.unwrap(),\n+            }),\n+            rewrite_speculation: Some(true),\n         },\n     )\n     .fuse();\n\n```\n\nUser edited \"crates/collab/src/llm.rs\":\n```diff\n@@ -493,7 +493,7 @@\n             max_tokens: 512,\n             temperature: 0.,\n             prediction: Some(fireworks::Prediction::Content {\n-                content: params.input_focus_excerpt.unwrap(),\n+                content: params.input_exc.unwrap(),\n             }),\n             rewrite_speculation: Some(true),\n         },\n\n```\n\n### Input:\n```crates/collab/src/llm.rs\n    Extension(claims): Extension<LlmTokenClaims>,\n<|editable_region_start|>\n    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,\n    Json(params): Json<PredictEditsParams>,\n) -> Result<impl IntoResponse> {\n    if !claims.is_staff && !claims.has_predict_edits_feature_flag {\n        return Err(Error::http(\n            StatusCode::FORBIDDEN,\n            \"no access to Zed's edit prediction feature\".to_string(),\n        ));\n    }\n\n    let api_url = state\n        .config\n        .prediction_api_url\n        .as_ref()\n        .context(\"no PREDICTION_API_URL configured on the server\")?;\n    let api_key = state\n        .config\n        .prediction_api_key\n        .as_ref()\n        .context(\"no PREDICTION_API_KEY configured on the server\")?;\n    let model = state\n        .config\n        .prediction_model\n        .as_ref()\n        .context(\"no PREDICTION_MODEL configured on the server\")?;\n\n    let outline_prefix = params\n        .outline\n        .as_ref()\n        .map(|outline| format!(\"### Outline for current file:\\n{}\\n\", outline))\n        .unwrap_or_default();\n\n    // todo(\"remove unwraps\")\n    let prompt = include_str!(\"./llm/prediction_prompt.md\")\n        .replace(\"<outline>\", &outline_prefix)\n        .replace(\"<events>\", &params.input_events)\n        .replace(\"<excerpt>\", &params.input_excerpt);\n\n    println!(\"Input Prompt: {}\", prompt);\n\n    let request_start = std::time::Instant::now();\n    let timeout = state\n        .executor\n        .sleep(std::time::Duration::from_secs(20))\n        .fuse();\n    let response = fireworks::complete(\n        &state.http_client,\n        api_url,\n        api_key,\n        fireworks::CompletionRequest {\n            model: model.to_string(),\n            stop: vec![\"<|editable_region_end|>\".to_string()],\n            prompt: prompt.clone(),\n            max_tokens: 512,\n            temperature: 0.,\n            prediction: Some(fireworks::Prediction::Content {\n                content: params.input_exc<|user_cursor_is_here|>.unwrap(),\n            }),\n            rewrite_speculation: Some(true),\n        },\n    )\n    .fuse();\n    futures::pin_mut!(timeout);\n    futures::pin_mut!(response);\n\n    futures::select! {\n        _ = timeout => {\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Timeout\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"prompt\": prompt,\n                        }),\n                    )\n                    .write(&kinesis_client, &kinesis_stream)\n                    .await\n                    .log_err();\n                }\n            });\n            Err(anyhow!(\"request timed out\"))?\n        },\n        response = response => {\n            let duration = request_start.elapsed();\n\n            let mut response = response?;\n            let choice = response\n                .completion\n                .choices\n                .pop()\n                .context(\"no output from completion response\")?;\n\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Requested\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"headers\": response.headers,\n                            \"usage\": response.completion.usage,\n<|editable_region_end|>\n                            \"headers\": response.headers,\n```\n\n### Response\n\n",
  "rewrite_speculation": true,
  "prediction": {
    "content": "```crates/collab/src/llm.rs\n    Extension(claims): Extension<LlmTokenClaims>,\n<|editable_region_start|>\n    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,\n    Json(params): Json<PredictEditsParams>,\n) -> Result<impl IntoResponse> {\n    if !claims.is_staff && !claims.has_predict_edits_feature_flag {\n        return Err(Error::http(\n            StatusCode::FORBIDDEN,\n            \"no access to Zed's edit prediction feature\".to_string(),\n        ));\n    }\n\n    let api_url = state\n        .config\n        .prediction_api_url\n        .as_ref()\n        .context(\"no PREDICTION_API_URL configured on the server\")?;\n    let api_key = state\n        .config\n        .prediction_api_key\n        .as_ref()\n        .context(\"no PREDICTION_API_KEY configured on the server\")?;\n    let model = state\n        .config\n        .prediction_model\n        .as_ref()\n        .context(\"no PREDICTION_MODEL configured on the server\")?;\n\n    let outline_prefix = params\n        .outline\n        .as_ref()\n        .map(|outline| format!(\"### Outline for current file:\\n{}\\n\", outline))\n        .unwrap_or_default();\n\n    // todo(\"remove unwraps\")\n    let prompt = include_str!(\"./llm/prediction_prompt.md\")\n        .replace(\"<outline>\", &outline_prefix)\n        .replace(\"<events>\", &params.input_events)\n        .replace(\"<excerpt>\", &params.input_excerpt);\n\n    println!(\"Input Prompt: {}\", prompt);\n\n    let request_start = std::time::Instant::now();\n    let timeout = state\n        .executor\n        .sleep(std::time::Duration::from_secs(20))\n        .fuse();\n    let response = fireworks::complete(\n        &state.http_client,\n        api_url,\n        api_key,\n        fireworks::CompletionRequest {\n            model: model.to_string(),\n            stop: vec![\"<|editable_region_end|>\".to_string()],\n            prompt: prompt.clone(),\n            max_tokens: 512,\n            temperature: 0.,\n            prediction: Some(fireworks::Prediction::Content {\n                content: params.input_exc<|user_cursor_is_here|>.unwrap(),\n            }),\n            rewrite_speculation: Some(true),\n        },\n    )\n    .fuse();\n    futures::pin_mut!(timeout);\n    futures::pin_mut!(response);\n\n    futures::select! {\n        _ = timeout => {\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Timeout\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"prompt\": prompt,\n                        }),\n                    )\n                    .write(&kinesis_client, &kinesis_stream)\n                    .await\n                    .log_err();\n                }\n            });\n            Err(anyhow!(\"request timed out\"))?\n        },\n        response = response => {\n            let duration = request_start.elapsed();\n\n            let mut response = response?;\n            let choice = response\n                .completion\n                .choices\n                .pop()\n                .context(\"no output from completion response\")?;\n\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Requested\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"headers\": response.headers,\n                            \"usage\": response.completion.usage,\n<|editable_region_end|>\n                            \"headers\": response.headers,\n```"
  },
  "max_tokens": 1024
}