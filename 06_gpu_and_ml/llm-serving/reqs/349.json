{
  "temperature": 0,
  "prompt": "## Task\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\n\n### Events:\n\n\n### Input:\n```crates/zeta/src/zeta.rs\n<|editable_region_start|>\n<|user_cursor_is_here|>mod completion_diff_element;\nmod rate_completion_modal;\n\npub(crate) use completion_diff_element::*;\npub use rate_completion_modal::*;\n\nuse anyhow::{anyhow, Context as _, Result};\nuse arrayvec::ArrayVec;\nuse client::{Client, UserStore};\nuse collections::{HashMap, HashSet, VecDeque};\nuse feature_flags::FeatureFlagAppExt as _;\nuse futures::AsyncReadExt;\nuse gpui::{\n    actions, App, AppContext as _, AsyncApp, Context, Entity, EntityId, Global, Subscription, Task,\n};\nuse http_client::{HttpClient, Method};\nuse language::{\n    language_settings::all_language_settings, Anchor, Buffer, BufferSnapshot, OffsetRangeExt,\n    Point, ToOffset, ToPoint,\n};\nuse language_models::LlmApiToken;\nuse rpc::{PredictEditsParams, PredictEditsResponse, EXPIRED_LLM_TOKEN_HEADER_NAME};\nuse std::{\n    borrow::Cow,\n    cmp,\n    fmt::Write,\n    future::Future,\n    mem,\n    ops::Range,\n    path::Path,\n    sync::Arc,\n    time::{Duration, Instant},\n};\nuse telemetry_events::InlineCompletionRating;\nuse util::ResultExt;\nuse uuid::Uuid;\n\nconst CURSOR_MARKER: &'static str = \"<|user_cursor_is_here|>\";\nconst START_OF_FILE_MARKER: &'static str = \"<|start_of_file|>\";\nconst EDITABLE_REGION_START_MARKER: &'static str = \"<|editable_region_start|>\";\nconst EDITABLE_REGION_END_MARKER: &'static str = \"<|editable_region_end|>\";\nconst BUFFER_CHANGE_GROUPING_INTERVAL: Duration = Duration::from_secs(1);\n\n// TODO(mgsloan): more systematic way to choose or tune these fairly arbitrary constants?\n\n/// Typical number of string bytes per token for the purposes of limiting model input. This is\n/// intentionally low to err on the side of underestimating limits.\nconst BYTES_PER_TOKEN_GUESS: usize = 3;\n\n/// This is based on the output token limit `max_tokens: 2048` in `crates/collab/src/llm.rs`. Number\n/// of output tokens is relevant to the size of the input excerpt because the model is tasked with\n/// outputting a modified excerpt. `2/3` is chosen so that there are some output tokens remaining\n/// for the model to specify insertions.\nconst BUFFER_EXCERPT_BYTE_LIMIT: usize = (2048 * 2 / 3) * BYTES_PER_TOKEN_GUESS;\n\n/// Note that this is not the limit for the overall prompt, just for the inputs to the template\n/// instantiated in `crates/collab/src/llm.rs`.\nconst TOTAL_BYTE_LIMIT: usize = BUFFER_EXCERPT_BYTE_LIMIT * 2;\n\n/// Maximum number of events to include in the prompt.\nconst MAX_EVENT_COUNT: usize = 16;\n\n/// Maximum number of string bytes in a single event. Arbitrarily choosing this to be 4x the size of\n/// equally splitting up the the remaining bytes after the largest possible buffer excerpt.\nconst PER_EVENT_BYTE_LIMIT: usize =\n    (TOTAL_BYTE_LIMIT - BUFFER_EXCERPT_BYTE_LIMIT) / MAX_EVENT_COUNT * 4;\n\nactions!(edit_prediction, [ClearHistory]);\n\n#[derive(Copy, Clone, Default, Debug, PartialEq, Eq, Hash)]\npub struct InlineCompletionId(Uuid);\n\nimpl From<InlineCompletionId> for gpui::ElementId {\n    fn from(value: InlineCompletionId) -> Self {\n        gpui::ElementId::Uuid(value.0)\n    }\n}\n\nimpl std::fmt::Display for InlineCompletionId {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(f, \"{}\", self.0)\n    }\n}\n\nimpl InlineCompletionId {\n    fn new() -> Self {\n        Self(Uuid::new_v4())\n    }\n}\n\n#[derive(Clone)]\nstruct ZetaGlobal(Entity<Zeta>);\n\nimpl Global for ZetaGlobal {}\n\n#[derive(Clone)]\npub struct InlineCompletion {\n    id: InlineCompletionId,\n    path: Arc<Path>,\n    excerpt_range: Range<usize>,\n    cursor_offset: usize,\n    edits: Arc<[(Range<Anchor>, String)]>,\n    snapshot: BufferSnapshot,\n    input_outline: Arc<str>,\n    input_events: Arc<str>,\n    input_excerpt: Arc<str>,\n    output_excerpt: Arc<str>,\n    request_sent_at: Instant,\n    response_received_at: Instant,\n}\n\nimpl InlineCompletion {\n    fn latency(&self) -> Duration {\n        self.response_received_at\n            .duration_since(self.request_sent_at)\n    }\n\n    fn interpolate(&self, new_snapshot: &BufferSnapshot) -> Option<Vec<(Range<Anchor>, String)>> {\n        let mut edits = Vec::new();\n<|editable_region_end|>\n    fn interpolate(&self, new_snapshot: &BufferSnapshot) -> Option<Vec<(Range<Anchor>, String)>> {\n```\n\n### Response\n\n",
  "rewrite_speculation": true,
  "prediction": {
    "content": "```crates/zeta/src/zeta.rs\n<|editable_region_start|>\n<|user_cursor_is_here|>mod completion_diff_element;\nmod rate_completion_modal;\n\npub(crate) use completion_diff_element::*;\npub use rate_completion_modal::*;\n\nuse anyhow::{anyhow, Context as _, Result};\nuse arrayvec::ArrayVec;\nuse client::{Client, UserStore};\nuse collections::{HashMap, HashSet, VecDeque};\nuse feature_flags::FeatureFlagAppExt as _;\nuse futures::AsyncReadExt;\nuse gpui::{\n    actions, App, AppContext as _, AsyncApp, Context, Entity, EntityId, Global, Subscription, Task,\n};\nuse http_client::{HttpClient, Method};\nuse language::{\n    language_settings::all_language_settings, Anchor, Buffer, BufferSnapshot, OffsetRangeExt,\n    Point, ToOffset, ToPoint,\n};\nuse language_models::LlmApiToken;\nuse rpc::{PredictEditsParams, PredictEditsResponse, EXPIRED_LLM_TOKEN_HEADER_NAME};\nuse std::{\n    borrow::Cow,\n    cmp,\n    fmt::Write,\n    future::Future,\n    mem,\n    ops::Range,\n    path::Path,\n    sync::Arc,\n    time::{Duration, Instant},\n};\nuse telemetry_events::InlineCompletionRating;\nuse util::ResultExt;\nuse uuid::Uuid;\n\nconst CURSOR_MARKER: &'static str = \"<|user_cursor_is_here|>\";\nconst START_OF_FILE_MARKER: &'static str = \"<|start_of_file|>\";\nconst EDITABLE_REGION_START_MARKER: &'static str = \"<|editable_region_start|>\";\nconst EDITABLE_REGION_END_MARKER: &'static str = \"<|editable_region_end|>\";\nconst BUFFER_CHANGE_GROUPING_INTERVAL: Duration = Duration::from_secs(1);\n\n// TODO(mgsloan): more systematic way to choose or tune these fairly arbitrary constants?\n\n/// Typical number of string bytes per token for the purposes of limiting model input. This is\n/// intentionally low to err on the side of underestimating limits.\nconst BYTES_PER_TOKEN_GUESS: usize = 3;\n\n/// This is based on the output token limit `max_tokens: 2048` in `crates/collab/src/llm.rs`. Number\n/// of output tokens is relevant to the size of the input excerpt because the model is tasked with\n/// outputting a modified excerpt. `2/3` is chosen so that there are some output tokens remaining\n/// for the model to specify insertions.\nconst BUFFER_EXCERPT_BYTE_LIMIT: usize = (2048 * 2 / 3) * BYTES_PER_TOKEN_GUESS;\n\n/// Note that this is not the limit for the overall prompt, just for the inputs to the template\n/// instantiated in `crates/collab/src/llm.rs`.\nconst TOTAL_BYTE_LIMIT: usize = BUFFER_EXCERPT_BYTE_LIMIT * 2;\n\n/// Maximum number of events to include in the prompt.\nconst MAX_EVENT_COUNT: usize = 16;\n\n/// Maximum number of string bytes in a single event. Arbitrarily choosing this to be 4x the size of\n/// equally splitting up the the remaining bytes after the largest possible buffer excerpt.\nconst PER_EVENT_BYTE_LIMIT: usize =\n    (TOTAL_BYTE_LIMIT - BUFFER_EXCERPT_BYTE_LIMIT) / MAX_EVENT_COUNT * 4;\n\nactions!(edit_prediction, [ClearHistory]);\n\n#[derive(Copy, Clone, Default, Debug, PartialEq, Eq, Hash)]\npub struct InlineCompletionId(Uuid);\n\nimpl From<InlineCompletionId> for gpui::ElementId {\n    fn from(value: InlineCompletionId) -> Self {\n        gpui::ElementId::Uuid(value.0)\n    }\n}\n\nimpl std::fmt::Display for InlineCompletionId {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(f, \"{}\", self.0)\n    }\n}\n\nimpl InlineCompletionId {\n    fn new() -> Self {\n        Self(Uuid::new_v4())\n    }\n}\n\n#[derive(Clone)]\nstruct ZetaGlobal(Entity<Zeta>);\n\nimpl Global for ZetaGlobal {}\n\n#[derive(Clone)]\npub struct InlineCompletion {\n    id: InlineCompletionId,\n    path: Arc<Path>,\n    excerpt_range: Range<usize>,\n    cursor_offset: usize,\n    edits: Arc<[(Range<Anchor>, String)]>,\n    snapshot: BufferSnapshot,\n    input_outline: Arc<str>,\n    input_events: Arc<str>,\n    input_excerpt: Arc<str>,\n    output_excerpt: Arc<str>,\n    request_sent_at: Instant,\n    response_received_at: Instant,\n}\n\nimpl InlineCompletion {\n    fn latency(&self) -> Duration {\n        self.response_received_at\n            .duration_since(self.request_sent_at)\n    }\n\n    fn interpolate(&self, new_snapshot: &BufferSnapshot) -> Option<Vec<(Range<Anchor>, String)>> {\n        let mut edits = Vec::new();\n<|editable_region_end|>\n    fn interpolate(&self, new_snapshot: &BufferSnapshot) -> Option<Vec<(Range<Anchor>, String)>> {\n```"
  },
  "max_tokens": 1024
}