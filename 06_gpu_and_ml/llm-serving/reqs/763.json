{
  "temperature": 0,
  "prompt": "## Task\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nYou are a code completion assistant and your task is to analyze user edits and then rewrite an excerpt that the user provides, suggesting the appropriate edits within the excerpt, taking into account the cursor location.\n\n### Events:\nUser edited \"untitled\":\n```diff\n@@ -207,4 +207,5 @@\n The kernel is crucial because it's the code that actually executes these operations on the GPU hardware. Without an efficient kernel, you could have a perfectly quantized model that still runs slowly because the execution isn't optimized.\n \n Think of it this way: Quantization is like compressing your files to save space, but the kernel is the program that actually knows how to efficiently use and process those compressed files. Both parts need to work well together for optimal performance.\n-I see, so when quantizing the model I am also \n\\ No newline at end of file\n+I see, so when quantizing the model I am also gonna use this Kernel at inference time?\n+\n\n```\n\n\n\nUser edited \"untitled\":\n```diff\n@@ -232,3 +232,5 @@\n The kernel is essentially the \"engine\" that runs your quantized model during inference - it handles all the complex operations needed to efficiently process the compressed weights while maintaining good performance.\n \n This is why having an efficient kernel like Machete is so important - even if your model is well-quantized, you need an efficient kernel to actually realize the performance benefits during inference.\n+Makes a lot of sense.\n+\n\n```\n\n\n\nUser edited \"script/gen-dataset\":\n```diff\n@@ -8,7 +8,7 @@\n const getFiles = (dir) => {\n   const files = fs.readdirSync(dir);\n   return files.reduce((acc, file) => {\n-    const dirFilePath = path.join(dir, file);\n+    const filePath = path.join(dir, file);\n     if (fs.statSync(filePath).isDirectory()) {\n       return acc.concat(getFiles(filePath));\n     }\n\n```\n\nUser edited \"script/gen-dataset\":\n```diff\n@@ -5,7 +5,7 @@\n const path = require(\"path\");\n \n // Get all .md files recursively\n-const getFiles = (dir) => {\n+const getFilesSassy = (dir) => {\n   const files = fs.readdirSync(dir);\n   return files.reduce((acc, file) => {\n     const filePath = path.join(dir, file);\n\n```\n\nUser edited \"script/gen-dataset\":\n```diff\n@@ -10,7 +10,7 @@\n   return files.reduce((acc, file) => {\n     const filePath = path.join(dir, file);\n     if (fs.statSync(filePath).isDirectory()) {\n-      return acc.concat(getFiles(filePath));\n+      return acc.concat(getFilesz(filePath));\n     }\n     if (path.extname(file) === \".md\") {\n       return acc.concat(filePath);\n\n```\n\nUser edited \"script/gen-dataset\":\n```diff\n@@ -5,12 +5,12 @@\n const path = require(\"path\");\n \n // Get all .md files recursively\n-const getFilesSassy = (dir) => {\n+const getFiles = (dir) => {\n   const files = fs.readdirSync(dir);\n   return files.reduce((acc, file) => {\n     const filePath = path.join(dir, file);\n     if (fs.statSync(filePath).isDirectory()) {\n-      return acc.concat(getFilesz(filePath));\n+      return acc.concat(getFiles(filePath));\n     }\n     if (path.extname(file) === \".md\") {\n       return acc.concat(filePath);\n\n```\n\nUser edited \"untitled\":\n```diff\n@@ -249,3 +249,4 @@\n - Better performance for small batch sizes\n \n These developments will likely make running large models even more efficient in the future.\n+What do you think --precision=FP8_MM_V2\n\\ No newline at end of file\n\n```\n\nUser edited \"untitled\":\n```diff\n@@ -249,4 +249,5 @@\n - Better performance for small batch sizes\n \n These developments will likely make running large models even more efficient in the future.\n-What do you think --precision=FP8_MM_V2\n\\ No newline at end of file\n+What do you think --precision=FP8_MM_V2 means?\n+\n\n```\n\n\n\nUser edited \"untitled\":\n```diff\n@@ -272,3 +272,5 @@\n - Hardware-accelerated performance on newer NVIDIA GPUs (especially H100s which have specific FP8 support)\n \n It's a middle ground between full precision (FP32/FP16) and more aggressive quantization like 4-bit, offering a balance of performance and accuracy.\n+This is VLLM isn't it\n+\n\n```\n\n### Input:\n```crates/collab/src/llm.rs\n    Extension(claims): Extension<LlmTokenClaims>,\n<|editable_region_start|>\n    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,\n    Json(params): Json<PredictEditsParams>,\n) -> Result<impl IntoResponse> {\n    if !claims.is_staff && !claims.has_predict_edits_feature_flag {\n        return Err(Error::http(\n            StatusCode::FORBIDDEN,\n            \"no access to Zed's edit prediction feature\".to_string(),\n        ));\n    }\n\n    let api_url = state\n        .config\n        .prediction_api_url\n        .as_ref()\n        .context(\"no PREDICTION_API_URL configured on the server\")?;\n    let api_key = state\n        .config\n        .prediction_api_key\n        .as_ref()\n        .context(\"no PREDICTION_API_KEY configured on the server\")?;\n    let model = state\n        .config\n        .prediction_model\n        .as_ref()\n        .context(\"no PREDICTION_MODEL configured on the server\")?;\n\n    let outline_prefix = params\n        .outline\n        .as_ref()\n        .map(|outline| format!(\"### Outline for current file:\\n{}\\n\", outline))\n        .unwrap_or_default();\n\n    // todo(\"remove unwraps\")\n    let prompt = include_str!(\"./llm/prediction_prompt.md\")\n        .replace(\"<outline>\", &outline_prefix)\n        .replace(\"<events>\", &params.input_events)\n        .replace(\"<excerpt>\", &params.input_excerpt);\n\n    println!(\"Input Prompt: {}\", prompt);\n\n    let request_start = std::time::Instant::now();\n    let timeout = state\n        .executor\n        .sleep(std::time::Duration::from_secs(20))\n        .fuse();\n    let response = fireworks::complete(\n        &state.http_client,\n        api_url,\n        api_key,\n        fireworks::CompletionRequest {\n            model: model.to_string(),\n            stop: vec![\"<|editable_region_end|>\".to_string()],\n            prompt: prompt.clone(),\n            max_tokens: 512,\n            temperature: 0.,\n            prediction: None,<|user_cursor_is_here|>\n            // prediction: Some(fireworks::Prediction::Content {\n            //     content: params.input_focus_excerpt.unwrap(),\n            // }),\n            // rewrite_speculation: Some(true),\n        },\n    )\n    .fuse();\n    futures::pin_mut!(timeout);\n    futures::pin_mut!(response);\n\n    futures::select! {\n        _ = timeout => {\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Timeout\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"prompt\": prompt,\n                        }),\n                    )\n                    .write(&kinesis_client, &kinesis_stream)\n                    .await\n                    .log_err();\n                }\n            });\n            Err(anyhow!(\"request timed out\"))?\n        },\n        response = response => {\n            let duration = request_start.elapsed();\n\n            let mut response = response?;\n            let choice = response\n                .completion\n                .choices\n                .pop()\n                .context(\"no output from completion response\")?;\n\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Requested\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"headers\": response.headers,\n<|editable_region_end|>\n                            \"model\": model.to_string(),\n```\n\n### Response\n\n",
  "rewrite_speculation": true,
  "prediction": {
    "content": "```crates/collab/src/llm.rs\n    Extension(claims): Extension<LlmTokenClaims>,\n<|editable_region_start|>\n    _country_code_header: Option<TypedHeader<CloudflareIpCountryHeader>>,\n    Json(params): Json<PredictEditsParams>,\n) -> Result<impl IntoResponse> {\n    if !claims.is_staff && !claims.has_predict_edits_feature_flag {\n        return Err(Error::http(\n            StatusCode::FORBIDDEN,\n            \"no access to Zed's edit prediction feature\".to_string(),\n        ));\n    }\n\n    let api_url = state\n        .config\n        .prediction_api_url\n        .as_ref()\n        .context(\"no PREDICTION_API_URL configured on the server\")?;\n    let api_key = state\n        .config\n        .prediction_api_key\n        .as_ref()\n        .context(\"no PREDICTION_API_KEY configured on the server\")?;\n    let model = state\n        .config\n        .prediction_model\n        .as_ref()\n        .context(\"no PREDICTION_MODEL configured on the server\")?;\n\n    let outline_prefix = params\n        .outline\n        .as_ref()\n        .map(|outline| format!(\"### Outline for current file:\\n{}\\n\", outline))\n        .unwrap_or_default();\n\n    // todo(\"remove unwraps\")\n    let prompt = include_str!(\"./llm/prediction_prompt.md\")\n        .replace(\"<outline>\", &outline_prefix)\n        .replace(\"<events>\", &params.input_events)\n        .replace(\"<excerpt>\", &params.input_excerpt);\n\n    println!(\"Input Prompt: {}\", prompt);\n\n    let request_start = std::time::Instant::now();\n    let timeout = state\n        .executor\n        .sleep(std::time::Duration::from_secs(20))\n        .fuse();\n    let response = fireworks::complete(\n        &state.http_client,\n        api_url,\n        api_key,\n        fireworks::CompletionRequest {\n            model: model.to_string(),\n            stop: vec![\"<|editable_region_end|>\".to_string()],\n            prompt: prompt.clone(),\n            max_tokens: 512,\n            temperature: 0.,\n            prediction: None,<|user_cursor_is_here|>\n            // prediction: Some(fireworks::Prediction::Content {\n            //     content: params.input_focus_excerpt.unwrap(),\n            // }),\n            // rewrite_speculation: Some(true),\n        },\n    )\n    .fuse();\n    futures::pin_mut!(timeout);\n    futures::pin_mut!(response);\n\n    futures::select! {\n        _ = timeout => {\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Timeout\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"prompt\": prompt,\n                        }),\n                    )\n                    .write(&kinesis_client, &kinesis_stream)\n                    .await\n                    .log_err();\n                }\n            });\n            Err(anyhow!(\"request timed out\"))?\n        },\n        response = response => {\n            let duration = request_start.elapsed();\n\n            let mut response = response?;\n            let choice = response\n                .completion\n                .choices\n                .pop()\n                .context(\"no output from completion response\")?;\n\n            state.executor.spawn_detached({\n                let kinesis_client = state.kinesis_client.clone();\n                let kinesis_stream = state.config.kinesis_stream.clone();\n                let model = model.clone();\n                async move {\n                    SnowflakeRow::new(\n                        \"Fireworks Completion Requested\",\n                        claims.metrics_id,\n                        claims.is_staff,\n                        claims.system_id.clone(),\n                        json!({\n                            \"model\": model.to_string(),\n                            \"headers\": response.headers,\n<|editable_region_end|>\n                            \"model\": model.to_string(),\n```"
  },
  "max_tokens": 1024
}